#!/bin/bash

# This SLURM script borrows some setup/teardown material from the SDSC script
# found in /util/academic/myhadoop/myHadoop-0.30b/examples.
# CDC January 29, 2015
#
# Modified 3/9/15 by Alex Foss for streaming mode
#
# Data files are assumed to have been processed by the preprocessing.slurm
# pipeline, as metadata generated by this pipeline is accessed by this script.

################################
# User-specified values
################################

# Debugging SLURM options
#SBATCH --partition=debug
#SBATCH --time=00:55:00
#SBATCH --nodes=2

# Full SLURM options
##SBATCH --partition=general-compute
##SBATCH --time=70:50:00
##SBATCH --nodes=10
##Specifies that the job will be requeued after a node failure.
##The default is that the job will not be requeued.
##SBATCH --requeue

# General SLURM options for full or debugging mode
## --exclusive allocates the entire node for this job
#SBATCH --exclusive
#SBATCH --job-name="test87"

# don't change this name without changing logger also
#SBATCH --output=kamilaResults-%J.out

# Email reporting options
##SBATCH --mail-user=youremail@host.com
##SBATCH --mail-type=ALL

# File to be clustered:
# Data file is ./$DATADIR/$DATANAME
DATANAME=1987_KAM_rmvna_norm.csv
# subsampled continuous variables e.g. as generated by py/subsampleSqlData.py
SUBSAMP_DATANAME=subsampled_2000_"$DATANAME"
CATINFO_DATANAME=1987_KAM_rmvna_catstats.tsv
# directory in which data files are stored
DATADIR=csv

# input seed for RNG
INITIAL_SEED=1234
# number of clusters to search for
NUM_CLUST=8
# Number of mappers
NUM_MAP=16 #80
# Number of reducers
NUM_REDUCE=16 #80
# The number of keys per cluster
# Currently select so that NUM_CLUST x NUM_CHUNK > NUM_REDUCE
NUM_CHUNK=3 #12

# Number of initializations of the kamila algorithm
NUM_INIT=2
# Max number of iterations per initialization
MAX_NITER=2
# Tolerances for determining convergence; see README.txt
# Converted to float
EPSILON_CON=0.01
EPSILON_CAT=0.01

export RBIN_HOME="/util/academic/R/R-3.0.0/bin"
echo "RBIN_HOME="$RBIN_HOME
################################
# End of user-specified values
################################

# Required packages
module load java/1.6.0_22
module load hadoop/2.5.1
module load myhadoop/0.30b
module load R/3.0.0
module list

# Log job stats in a central file
mkdir -p logs
MASTER_LOG=logs/jobLog.tsv
if [ ! -f "$MASTER_LOG" ]
  then echo "Creating master log file: ""$MASTER_LOG"
  echo -e "JobId\t"\
"DataName\t"\
"Seed\t"\
"NominalNumMap\t"\
"NominalNumRed\t"\
"NumNode\t"\
"NumClust\t"\
"NumChunk\t"\
"MapTime\t"\
"RedTime\t"\
"TotalTime\t"\
"ActualNumMap\t"\
"ActualNumRed\t"\
"FailStatus" > $MASTER_LOG
fi

echo "SLURM_JOBID="$SLURM_JOBID
echo "SLURM_JOB_NODELIST"=$SLURM_JOB_NODELIST
echo "SLURM_NNODES"=$SLURM_NNODES
echo "SLURMTMPDIR="$SLURMTMPDIR

SLURM_OUTFILE="kamilaResults-"$SLURM_JOBID".out"
echo "Logfile of this kamila implementation = "$SLURM_OUTFILE

echo "working directory = "$SLURM_SUBMIT_DIR

TMP_DIR=tmp_"$SLURM_JOBID"
echo "MH_HOME="$MH_HOME
echo "HADOOP_HOME="$HADOOP_HOME
echo "Setting HADOOP to use SLURMTMPDIR on the local disk"
export MH_SCRATCH_DIR=$SLURMTMPDIR
echo "MH_SCRATCH_DIR="$MH_SCRATCH_DIR
#### Set this to the directory where Hadoop configs should be generated
# Don't change the name of this variable (HADOOP_CONF_DIR) as it is
# required by Hadoop - all config files will be picked up from here
#
# Make sure that this is accessible to all nodes
export HADOOP_CONF_DIR=$SLURM_SUBMIT_DIR/config-kamila-$SLURM_JOBID
export HADOOP_RES_DIR=./output-kamila-$SLURM_JOBID
echo "MyHadoop config directory="$HADOOP_CONF_DIR
echo "MyHadoop results directory="$HADOOP_RES_DIR

echo INITIAL_SEED: $INITIAL_SEED
echo NUM_INIT: $NUM_INIT
echo MAX_NITER: $MAX_NITER
echo NUM_MAP: $NUM_MAP
echo NUM_REDUCE: $NUM_REDUCE
echo NUM_CLUST: $NUM_CLUST
echo NUM_CHUNK: $NUM_CHUNK
echo EPSILON_CON: $EPSILON_CON
echo EPSILON_CAT: $EPSILON_CAT
APPRX_KEYS=`expr $NUM_CLUST \* $NUM_CHUNK`
echo There will be about NUM_CHUNK x NUM_CLUST keys \(about $APPRX_KEYS\)
echo


### Set up the configuration
# Make sure number of nodes is the same as what you have requested from SLURM
# usage: $myhadoop-configure.sh -h
# this is the non-persistent mode
NPROCS=`srun --nodes=${SLURM_NNODES} bash -c 'hostname' |wc -l`
echo "-------Set up the configurations for myHadoop"
$MH_HOME/bin/myhadoop-configure.sh 
echo "-------Start hdfs and yarn ---"
$HADOOP_HOME/sbin/start-all.sh
#### Format HDFS, if this is the first time or not a persistent instance
echo "-------Show Report ---"
$HADOOP_HOME/bin/hadoop dfsadmin -report
echo "-------make directory ---"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -mkdir /data

echo "-------copy file to hdfs, don't copy header row ---"
#$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -put ./"$DATADIR"/"$DATANAME" /data/
sed 1d ./"$DATADIR"/"$DATANAME" | $HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -put - /data/"$DATANAME"

echo "-------list directory ---"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls /data

# parsing:
# Robj is an arbitrary object to be stored.
# It is stored in this manner:
# storedObj <- deparse(Robj)
#
# It can be regenerated as follows:
# reconstitutedObj <- eval(parse(text=storedObj))

# Design for kamila:
# input csv dataset, no cluster memberships, and RData file with initial centers
# Map/Reduce 1: calculate KDE of min distances
# Map step 2: each data row is assigned a cluster
#	output: <k \t csv data row>
# Reduce step 2: each cluster is assigned a centroid
#	output: <k \t deparsed R object describing cluster centroid>
# Intermediary file after 2nd M-R step converts deparsed data to RData file
# Each kamila run terminates after MAX_NITER iterations or when the centroids
#	change by less than EPSILON_CON and EPSILON_CAT units; whichever
#	happens first. See README.txt for details.


##################################################
##################################################
##################################################
## Outer loop starts here; each iteration is a unique
## kamila run. The best is chosen as the final
## partition.
##################################################
# create an array to store quality of each run's clustering
declare -a clusterQualityMetric

# Initialize seed and associated objects for pseudorandom RNG
mkdir $TMP_DIR

Rscript R/kamila/kamila_initialize_all.R \
    $INITIAL_SEED \
    $TMP_DIR \
    ./"$DATADIR"/"$SUBSAMP_DATANAME" \
    ./"$DATADIR"/"$CATINFO_DATANAME"

for ((runNumber=1; runNumber<=NUM_INIT; runNumber++))
do
    echo "-----------------------------------------------"
    echo "----------start kamila run $runNumber-------------------"
    echo "-----------------------------------------------"
    mkdir -p ./$HADOOP_RES_DIR/run_$runNumber/

    # Initialize log file for parameter differences from ith and (i-1)th iterations
    # Used to assess convergence
    echo "i,condiff,catdiff" > $TMP_DIR/paramDiff.csv

    # initialize the means based on the dimensionality of the data
    Rscript R/kamila/kamila_initialize_run.R $NUM_CLUST $TMP_DIR
    mkdir -p ./$HADOOP_RES_DIR/run_$runNumber/iter_0/
    cp $TMP_DIR/initialMeans.RData \
        ./$HADOOP_RES_DIR/run_$runNumber/iter_0/currentMeans_i1.RData
    
    for i in `seq 1 $MAX_NITER`
    do
        echo "--------ITER $i--------"
    
        echo "---M-R stage1---"
        # First M-R step: min dist KDE construction
        $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR \
          jar /util/academic/hadoop/2.5.1/hadoop-2.5.1/share/hadoop/tools/lib/hadoop-streaming-2.5.1.jar \
          -D mapred.map.tasks=$NUM_MAP \
          -D mapred.reduce.tasks=$NUM_REDUCE \
          -D mapred.text.key.comparator.options="-n" \
          -mapper "$RBIN_HOME/Rscript $PWD/R/kamila/kamila_mapper_1.R $NUM_CHUNK" \
          -reducer "$RBIN_HOME/Rscript $PWD/R/kamila/kamila_reducer_1.R" \
          -input /data/"$DATANAME" \
          -output hadoop_output_r"$runNumber"_i"$i"/stage1 \
          -file "$TMP_DIR"/currentMeans.RData
    
        echo "-------list output ---"
        # Get name of current output file
        $HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls hadoop_output_r"$runNumber"_i$i/stage1/
      
        # Copy output to local dir
        mkdir -p ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage1/
        $HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -get \
          hadoop_output_r"$runNumber"_i$i/stage1/part-* \
          ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage1
    
        echo Contents of $HADOOP_RES_DIR/run_$runNumber/iter_$i/stage1
        ls -lht ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage1

	# Calculate radial KDE
        cat ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage1/part-* | sort -t . -k1,1n -k2,2n | Rscript R/kamila/kamila_intermediary_1.R $TMP_DIR

        echo "---M-R stage2---"
        # Second M-R step: Cluster allocation and centroid re-estimation.
        $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR \
          jar /util/academic/hadoop/2.5.1/hadoop-2.5.1/share/hadoop/tools/lib/hadoop-streaming-2.5.1.jar \
          -D mapred.map.tasks=$NUM_MAP \
          -D mapred.reduce.tasks=$NUM_REDUCE \
          -D mapred.text.key.comparator.options="-n" \
          -mapper "$RBIN_HOME/Rscript $PWD/R/kamila/kamila_mapper_2.R $NUM_CHUNK" \
          -reducer "$RBIN_HOME/Rscript $PWD/R/kamila/kamila_reducer_2.R" \
          -input /data/"$DATANAME" \
          -output hadoop_output_r"$runNumber"_i"$i"/stage2 \
          -file "$TMP_DIR"/currentMeans.RData \
	  -file "$PWD"/R/helperFunctions.R

        echo "-------list output ---"
        # Get name of current output file
        $HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls hadoop_output_r"$runNumber"_i$i/stage2/
      
        # Copy output to local dir
        mkdir -p ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage2/
        $HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -get \
          hadoop_output_r"$runNumber"_i$i/stage2/part-* \
          ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage2
    
        echo Contents of $HADOOP_RES_DIR/run_$runNumber/iter_$i/stage2
        ls -lht ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage2
        echo Concatenated centroid chunk sum/count vectors:
	# -t . option specifies . as delimiter
	# -kx,xn specifies numeric sort in field x
        cat ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage2/part-* | sort -t . -k1,1n -k2,2n

        # Run script on output; kamila_intermediary_2.R outputs currentMeans.RData, as well
        # as the objective function for the current iteration.

        # Logs current parameter differences in $TMP_DIR/paramDiff.csv
        cat ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/stage2/part-* | \
	  sort -t . -k1,1n -k2,2n | \
	  Rscript R/kamila/kamila_intermediary_2.R $SLURM_JOBID $runNumber $i $TMP_DIR >> \
	  $TMP_DIR/paramDiff.csv

        echo "Current value of objective array is:"
	head -1 $TMP_DIR/paramDiff.csv 
	tail -1 $TMP_DIR/paramDiff.csv 
        cp $TMP_DIR/paramDiff.csv ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/currentObjective_i$(expr $i + 1).csv
        cp $TMP_DIR/currentMeans.RData ./$HADOOP_RES_DIR/run_$runNumber/iter_$i/currentMeans_i$(expr $i + 1).RData
      
        # check if convergence criteria is met
	paramDiffCon=`tail -1 $TMP_DIR/paramDiff.csv | awk -F',' '{print $2}'`
	paramDiffCat=`tail -1 $TMP_DIR/paramDiff.csv | awk -F',' '{print $3}'`
	regex='^[0-9]+([.][0-9]+)?$'
	if ! [[ $paramDiffCon =~ $regex ]]
	then
	  echo "Error: paramDiffCon ("$paramDiffCon") is not a number" >&2
          echo "-------Stop hdfs and yarn ---"
          $HADOOP_HOME/sbin/stop-all.sh
          #### Clean up the working directories after job completion
          $MH_HOME/bin/myhadoop-cleanup.sh
	  exit 1
	fi
	if ! [[ $paramDiffCat =~ $regex ]]
	then
	  echo "Error: paramDiffCat ("$paramDiffCat") is not a number" >&2
          echo "-------Stop hdfs and yarn ---"
          $HADOOP_HOME/sbin/stop-all.sh
          #### Clean up the working directories after job completion
          $MH_HOME/bin/myhadoop-cleanup.sh
	  exit 1
	fi

        if (( $(bc <<< "$paramDiffCon < $EPSILON_CON && $paramDiffCat < $EPSILON_CAT") ))
        then
          # if convergence criteria is met, then break this kamila run
          echo "Convergence criteria met: Breaking kamila run early"
          break
        fi
    done
    
    # end of this kamila iter
    rm $TMP_DIR/paramDiff.csv

    ##################################################
    echo "Summary stats M-R run:"
    ##################################################
    #
    # Final map-reduce run to calculate within cluster
    # stats and final cluster stats.
    #
    # M: calc cluster memberships
    # R: cluster counts
    #    WSS, min, total, count, max for continuous
    #    logLik, cell counts for categorical
    #
    ##################################################
    $HADOOP_HOME/bin/hadoop --config $HADOOP_CONF_DIR \
      jar /util/academic/hadoop/2.5.1/hadoop-2.5.1/share/hadoop/tools/lib/hadoop-streaming-2.5.1.jar \
      -D mapred.map.tasks=$NUM_MAP \
      -D mapred.reduce.tasks=$NUM_REDUCE \
      -D mapred.text.key.comparator.options="-n" \
      -mapper "$RBIN_HOME/Rscript $PWD/R/kamila/kamila_summary_mapper.R $NUM_CHUNK" \
      -reducer "$RBIN_HOME/Rscript $PWD/R/kamila/kamila_summary_reducer.R" \
      -input /data/"$DATANAME" \
      -output run_"$runNumber"/hadoop_stats \
      -file "$TMP_DIR"/currentMeans.RData \
      -file "$PWD"/R/helperFunctions.R

    mkdir ./$HADOOP_RES_DIR/run_$runNumber/stats/
    $HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -get \
      run_"$runNumber"/hadoop_stats/part-* \
      ./$HADOOP_RES_DIR/run_$runNumber/stats/

    ## save stats file
    #echo  -e 'ClusterNumber\tClusterSize\tSSQ\tminvec\tsumvec\tmaxvec'  > \
    #  ./$HADOOP_RES_DIR/run_$runNumber/stats/chunkStats.tsv
    #cat ./$HADOOP_RES_DIR/run_$runNumber/stats/part-* | sort -t . -k1,1n -k2,2n \
    #  >> ./$HADOOP_RES_DIR/run_$runNumber/stats/chunkStats.tsv
    echo 
    echo "Calculating summary stats from summary M-R run"
    echo 
    cat ./$HADOOP_RES_DIR/run_$runNumber/stats/part-* | Rscript ./R/kamila/kamila_summary_intermediary.R \
      ./$HADOOP_RES_DIR/run_$runNumber/stats/
    
############################################################
## Updated from kmeans to kamila up to here ################
############################################################
    
    # calculate and store cluster quality
    clusterQualityMetric[$(($runNumber-1))]=`Rscript -e \
      "load('./$HADOOP_RES_DIR/run_$runNumber/stats/finalRunStats.RData'); cat(runSummary[['clusterQualityMetric']])"`
    echo "Clustering metric for this run is "${clusterQualityMetric[$(($runNumber-1))]}
    ##################################################
    echo "End of summary stats M-R run"
    ##################################################

    # remove tmp dir containing intial and current means
    # remove cluster files from tmp dir, but leave random seed file.
    ls -lht $TMP_DIR
    rm $TMP_DIR/{current,initial}Means.RData
    ls -lht $TMP_DIR

## outer number init kamila loop ends here
done
rm -r $TMP_DIR

# write out cluster quality for all runs
mkdir ./$HADOOP_RES_DIR/best_run
echo ${clusterQualityMetric[*]} > ./$HADOOP_RES_DIR/best_run/allClustQual.txt

# gather and write out stats on first MR run to file
FAIL_STATUS=`sed '/list output/q' $SLURM_OUTFILE | grep "Streaming Command Failed" | awk '{print $3}'`
ACTUAL_NUM_MAP=`sed '/list output/q' $SLURM_OUTFILE | grep "Launched map tasks" | awk -F'=' '{print $2}'`
ACTUAL_NUM_RED=`sed '/list output/q' $SLURM_OUTFILE | grep "Launched reduce tasks" | awk -F'=' '{print $2}'`
BYTES_READ=`grep -m1 'Number of bytes read=' $SLURM_OUTFILE | awk -F'=' '{print $2}'`

MAP_T1=`grep -m1 'map 0%' $SLURM_OUTFILE | awk -F' ' '{print $1 "-" $2}'`
MAP_T2=`grep -m1 'map 100%' $SLURM_OUTFILE | awk -F' ' '{print $1 "-" $2}'`
MAP_TIME=`python py/timeDiff.py $MAP_T1 $MAP_T2`
echo "MAP_T1"
echo $MAP_T1
echo "MAP_T2"
echo $MAP_T2
echo "MAP_TIME"
echo $MAP_TIME

RED_T1=`grep -m1 'reduce 0%' $SLURM_OUTFILE | awk -F' ' '{print $1 "-" $2}'`
RED_T2=`grep -m1 'reduce 100%' $SLURM_OUTFILE | awk -F' ' '{print $1 "-" $2}'`
RED_TIME=`python py/timeDiff.py $RED_T1 $RED_T2`
echo "RED_T1"
echo $RED_T1
echo "RED_T2"
echo $RED_T2
echo "RED_TIME"
echo $RED_TIME

OVERALL_T1=`grep -E -m1 '[[:digit:]]{2}/[[:digit:]]{2}/[[:digit:]]{2} [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}' $SLURM_OUTFILE | awk -F' ' '{print $1 "-" $2}'`
OVERALL_T2=`grep -E '[[:digit:]]{2}/[[:digit:]]{2}/[[:digit:]]{2} [[:digit:]]{2}:[[:digit:]]{2}:[[:digit:]]{2}' $SLURM_OUTFILE | tail -1 | awk -F' ' '{print $1 "-" $2}'`
OVERALL_TIME=`python py/timeDiff.py $OVERALL_T1 $OVERALL_T2`
echo "OVERALL_T1"
echo $OVERALL_T1
echo "OVERALL_T2"
echo $OVERALL_T2
echo "OVERALL_TIME"
echo $OVERALL_TIME


echo -e "$SLURM_JOBID\t"\
"$DATANAME\t"\
"$INITIAL_SEED\t"\
"$NUM_MAP\t"\
"$NUM_REDUCE\t"\
"$SLURM_NNODES\t"\
"$NUM_CLUST\t"\
"$NUM_CHUNK\t"\
"$MAP_TIME\t"\
"$RED_TIME\t"\
"$OVERALL_TIME\t"\
"$ACTUAL_NUM_MAP\t"\
"$ACTUAL_NUM_RED\t"\
"$FAIL_STATUS" >> $MASTER_LOG

echo
echo
echo
echo "-------list output ---"
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls 
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls hadoop_output_r1_i1
$HADOOP_HOME/bin/hdfs --config $HADOOP_CONF_DIR dfs -ls /data

echo "-------Stop hdfs and yarn ---"
$HADOOP_HOME/sbin/stop-all.sh

#### Clean up the working directories after job completion
$MH_HOME/bin/myhadoop-cleanup.sh

